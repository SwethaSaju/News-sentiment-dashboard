# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HS6vgS-l3skXkQc8pFKjrOF4QzZTPijH
"""


# ----------------------------
# app.py : Real-Time News Sentiment Dashboard
# ----------------------------
import requests
import pandas as pd
import time
import streamlit as st
from pyspark.sql import SparkSession
from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline

# ----------------------------
# 1. Setup API
# ----------------------------
API_KEY = "aa9f72d6bad1c3ffbd42dcba1e7b7ce5"

def fetch_news():
    url = f"https://gnews.io/api/v4/top-headlines?token={API_KEY}&lang=en&country=us&max=10"
    response = requests.get(url).json()
    articles = response.get("articles", [])
    data = []
    for art in articles:
        data.append({
            "headline": art["title"],
            "description": art["description"],
            "publishedAt": art["publishedAt"]
        })
    return pd.DataFrame(data)

# ----------------------------
# 2. Initialize Spark
# ----------------------------
spark = SparkSession.builder \
    .appName("RealTimeNewsSentiment") \
    .getOrCreate()

# ----------------------------
# 3. Training Data (Dummy Example)
# Replace with proper labeled dataset for real project
# ----------------------------
train_data = spark.createDataFrame([
    ("Stock market hits record high", 1),
    ("Economic crisis causes panic selling", 0),
    ("New vaccine brings hope to millions", 1),
    ("Natural disaster destroys thousands of homes", 0),
], ["headline", "label"])

# ----------------------------
# 4. Build ML Pipeline
# ----------------------------
tokenizer = Tokenizer(inputCol="headline", outputCol="words")
stopwords_remover = StopWordsRemover(inputCol="words", outputCol="filtered")
hashingTF = HashingTF(inputCol="filtered", outputCol="rawFeatures")
idf = IDF(inputCol="rawFeatures", outputCol="features")
lr = LogisticRegression(featuresCol="features", labelCol="label")

pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashingTF, idf, lr])
model = pipeline.fit(train_data)

# ----------------------------
# 5. Streamlit Dashboard
# ----------------------------
st.set_page_config(page_title="Real-Time News Sentiment", layout="wide")
st.title("ðŸ“° Real-Time News Sentiment Dashboard")

placeholder = st.empty()

while True:
    df_news = fetch_news()
    if not df_news.empty:
        spark_df = spark.createDataFrame(df_news)
        predictions = model.transform(spark_df)
        results = predictions.select("headline", "prediction").toPandas()

        # Map predictions to labels
        results["Sentiment"] = results["prediction"].apply(lambda x: "Positive" if x == 1.0 else "Negative")

        with placeholder.container():
            st.subheader("Latest Headlines with Sentiment")
            st.table(results[["headline", "Sentiment"]])

            # Show sentiment counts
            sentiment_counts = results["Sentiment"].value_counts()
            st.bar_chart(sentiment_counts)
    else:
        st.warning("No news fetched.")

    time.sleep(60)  # refresh every 60s
